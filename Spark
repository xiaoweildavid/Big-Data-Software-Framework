##############################################################################
# Input data under Spark environment (Cloudera, ver 1.6.0) 
# Use pre-stored database 'retail_db' as example.
# First, validate the database in MySQL.

mysql -u root -p
SHOW databases;
USE retail_db;
SHOW tables;

# Here tables 'orders' and 'order_items' will be used.
SELECT * FROM orders LIMIT 2;
SELECT * FROM order_items LIMIT 2;

# Now transfer the data to HIVE.
sqoop import \
--connect jdbc:mysql://localhost:3306/retail_db \
--username root \
--password cloudera \
--table orders \
--target-dir=/user/hive/warehouse/orders \
--hive-import

sqoop import \
--connect jdbc:mysql://localhost:3306/retail_db \
--username root \
--password cloudera \
--table order_items \
--target-dir=/user/hive/warehouse/order_items \
--hive-import

# Need to double-check import 2 tables simutaneously.
hdfs dfs -ls /user/hive/warehouse/orders/*
hdfs dfs -ls /user/hive/warehouse/order_items/*

# Validating tables under HIVE.
# Enter pyspark

from pyspark import SQLContext
sqlContext = SQLContext(sc)

# Create RDD of HIVE tables.
ordersRDD = sc.textFile('/user/hive/warehouse/orders/*')
order_itmesRDD = sc.textFile('/user/hive/warehouse/order_items/*')

# Validating loaded HIVE tables.
ordersRDD.take(3)
order_itemsRDD.take(3)

# Also, pyspark.sql.DataFrames can be created as well. Column names will be imported.
sqlContext = HiveContext(sc)
ordersDF = sqlContext.sql('SELECT * FROM orders')
orderItemsDF = sqlContext.sql('SELECT * FROM order_items')
type(df)

# Join two tables (or say DataFrames).
joinDF = sqlContext.sql('SELECT * FROM orders LEFT JOIN order_items ON orders.order_id=order_items.order_item_id')

# Get column values of a DF, cannot use DF['col'].
order_id = joinDF.select('order_id')

# Save DF to HDFS as JSON extension.
ordersDF.write.format('JSON').save(path='/user/cloudera/ordersDF')

# Import HDFS table files as RDD under pyspark.
ordersRDD = sc.textFile('/user/cloudera/orders/') # This is an example for textFiles, there are also jsonFiles, binaryFiles.
# RDD to Hive Temp Table.
?

# Pyspark DataFrame to RDD.
ordersRDD = ordersDF.rdd









