##############################################################################
# Input data under Spark environment (Cloudera, ver 1.6.0) 
# Use pre-stored database 'retail_db' as example.
# First, validate the database in MySQL.

mysql -u root -p
SHOW databases;
USE retail_db;
SHOW tables;

# Here tables 'orders' and 'order_items' will be used.
SELECT * FROM orders LIMIT 2;
SELECT * FROM order_items LIMIT 2;

# Now transfer the data to HIVE.
sqoop import \
--connect jdbc:mysql://localhost:3306/retail_db \
--username root \
--password cloudera \
--table orders \
--target-dir=/user/hive/warehouse/orders \
--hive-import

sqoop import \
--connect jdbc:mysql://localhost:3306/retail_db \
--username root \
--password cloudera \
--table order_items \
--target-dir=/user/hive/warehouse/order_items \
--hive-import

# Need to double-check import 2 tables simutaneously.
hdfs dfs -ls /user/hive/warehouse/orders/*
hdfs dfs -ls /user/hive/warehouse/order_items/*

# Validating tables under HIVE.
# Enter pyspark

from pyspark import SQLContext
sqlContext = SQLContext(sc)

# Create RDD of HIVE tables.
ordersRDD = sc.textFile('/user/hive/warehouse/orders/*')
order_itmesRDD = sc.textFile('/user/hive/warehouse/order_items/*')

# Validating loaded HIVE tables.
ordersRDD.take(3)
order_itemsRDD.take(3)
# Or
ordersRDD.show(3) # This format is more friendly.

# Also, pyspark.sql.DataFrames can be created as well. Column names will be imported.
sqlContext = HiveContext(sc)
ordersDF = sqlContext.sql('SELECT * FROM orders')
orderItemsDF = sqlContext.sql('SELECT * FROM order_items')
type(df)

# Join two tables (or say DataFrames).
joinDF = sqlContext.sql('SELECT * FROM orders LEFT JOIN order_items ON orders.order_id=order_items.order_item_id')

# Get column values of a DF, cannot use DF['col'].
order_id = joinDF.select('order_id')

# Save DF to HDFS as JSON extension.
ordersDF.write.format('JSON').save(path='/user/cloudera/ordersDF')

# Import HDFS table files as RDD under pyspark.
ordersRDD = sc.textFile('/user/cloudera/orders/') # This is an example for textFiles, there are also jsonFiles, binaryFiles.

# DF to Hive Temp Table.
ordersDF.registerTempTable('orders_tt')
# Validating
HQL.sql('SELECT * FROM orders_tt').show(10) # Output the first 10 records.

# Pyspark DataFrame to RDD.
ordersRDD = ordersDF.rdd

************************************************************************

# Operation on Hive by PySpark, using table retail_db.orders as example:
from pyspark import SQLContext, HiveContext
SQL = SQLContext(sc) # sc is the original spark context generated when pyspark starts.
HQL = HiveContext(sc)

# Create a table order_num_sum of customers and their total order numbers:
HQL.sql('CREATE TABLE order_num_sum AS \
SELECT order_customer_id, count(order_customer_id) as order_numbers FROM orders \
GROUP BY order_customer_id ORDER BY order_numbers DESC')

# Or if you want to see the generated table in PySpark:
HQL.sql('SELECT * FROM order_num_sum ORDER BY order_numbers DESC LIMIT 10').show()

# Save created table in different file formats:
orderNumSumDF = HQL.sql('SELECT * FROM order_num_sum')
orderNumSumDF.show(10)

# Output the table as a test file into HDFS.
orderNumSumDF.rdd.saveAsTextFile('/user/cloudera/fold') # The fold path cannot be pre-existed.

# Output the DF as a avro format:
orderNumSumDF.write.format("com.databricks.spark.avro").save("/user/cloudera/orderNumSum")

# Or as parquet format:
orderNumSumDF.write.format("parquet").save("/user/cloudera/orderNumSumParquet")



